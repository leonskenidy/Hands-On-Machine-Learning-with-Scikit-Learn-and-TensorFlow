{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Chapter 15 – Autoencoders**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To support both python 2 and python 3\n",
    "from __future__ import division, print_function, unicode_literals\n",
    "\n",
    "# Common imports\n",
    "import numpy as np\n",
    "import os\n",
    "import sys\n",
    "\n",
    "# to make this notebook's output stable across runs\n",
    "def reset_graph(seed=42):\n",
    "    tf.reset_default_graph()\n",
    "    tf.set_random_seed(seed)\n",
    "    np.random.seed(seed)\n",
    "\n",
    "# To plot pretty figures\n",
    "%matplotlib inline\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams['axes.labelsize'] = 14\n",
    "plt.rcParams['xtick.labelsize'] = 12\n",
    "plt.rcParams['ytick.labelsize'] = 12\n",
    "\n",
    "# Where to save the figures\n",
    "PROJECT_ROOT_DIR = \".\"\n",
    "CHAPTER_ID = \"autoencoders\"\n",
    "\n",
    "def save_fig(fig_id, tight_layout=True):\n",
    "    path = os.path.join(PROJECT_ROOT_DIR, \"images\", CHAPTER_ID, fig_id + \".png\")\n",
    "    print(\"Saving figure\", fig_id)\n",
    "    if tight_layout:\n",
    "        plt.tight_layout()\n",
    "    plt.savefig(path, format='png', dpi=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_image(image, shape=[28, 28]):\n",
    "    plt.imshow(image.reshape(shape), cmap=\"Greys\", interpolation=\"nearest\")\n",
    "    plt.axis(\"off\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_multiple_images(images, n_rows, n_cols, pad=2):\n",
    "    images = images - images.min()  # make the minimum == 0, so the padding looks white\n",
    "    w,h = images.shape[1:]\n",
    "    image = np.zeros(((w+pad)*n_rows+pad, (h+pad)*n_cols+pad))\n",
    "    for y in range(n_rows):\n",
    "        for x in range(n_cols):\n",
    "            image[(y*(h+pad)+pad):(y*(h+pad)+pad+h),(x*(w+pad)+pad):(x*(w+pad)+pad+w)] = images[y*n_cols+x]\n",
    "    plt.imshow(image, cmap=\"Greys\", interpolation=\"nearest\")\n",
    "    plt.axis(\"off\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "自动编码器是能够在无须任何监督（例如，训练数据集没有任何标记）的情况下学习有效表示输入数据（称为编码）的人工神经网络。这种编码通常比输入数据的维度低得多，使得自动编码可以用来降低维度（见第8章）。更重要的是，自动编码器作为强大的特征检测器，可以用于深层神经网络的无监督训练（如第11章所讨论的）。最后，它们能够生产与训练数据非常相似的新数据；这被称为生成模型。例如，使用面部图片训练自动编码器，然后它可以生成新的面部图片。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "与记忆试验中的围棋选手一样，自动编码器查看输入，将它们转化为有效的内部表示，然后输出一些看起来和输入很像的东西。一个自动编码器总是由两部分组成：第一部分是将输入转化为内部表示的编码器（或者称为识别网络），第二部分是将内部表示转换为输出的解码器（或者称为生成网络），见图15-1。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='images\\autoencoders\\The chess memory experiment (left) and a simple autoencoder (right).png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "图15-1：国际象棋记忆实验（左）和一个简单的自动编码器（右）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "自动编码器通常和多层感知器（MLP，参见第10章）具有相同的架构，不同之处在于自动编码器的输出层的神经元数量必须等于输入层的数量。在这个例子中，只有一个由两个神经元（编码器）组成的隐藏层，以及一个由三个神经元（解码器）组成的输出层。输出通常被称为重建，因为自动编码器尝试重建输入，并且成本函数包含重建损失，当重建与输入不同时，该损失会惩罚模型。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "因为内部表示的维度低于输入数据（它是2D而不是3D），因此自动编码器不完整。不完整的自动编码器不能简单地复制输入到编码，但是它必须找到一种输出其输入副本的方法。它被强制学习输入数据中最重要的功能（并删除不重要的功能）。让我们看看如何实现一个用来降低维度的非常简单的不完整自动编码器。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 使用不完整的线性自动编码器实现PCA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "如果自动编码器仅使用线性激活，并且成本函数是均方误差（MSE），则表示它可以用来实现主要组件分析（见第8章）。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "以下代码构建了一个简单的线性自动编码器，用于执行PCA，将3D数据集投影为2D："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build 3D dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy.random as rnd\n",
    "\n",
    "rnd.seed(4)\n",
    "m = 200\n",
    "w1, w2 = 0.1, 0.3\n",
    "noise = 0.1\n",
    "\n",
    "angles = rnd.rand(m) * 3 * np.pi / 2 - 0.5\n",
    "data = np.empty((m, 3))\n",
    "data[:, 0] = np.cos(angles) + np.sin(angles)/2 + noise * rnd.randn(m) / 2\n",
    "data[:, 1] = np.sin(angles) * 0.7 + noise * rnd.randn(m) / 2\n",
    "data[:, 2] = data[:, 0] * w1 + data[:, 1] * w2 + noise * rnd.randn(m)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Normalize the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(data[:100])\n",
    "X_test = scaler.transform(data[100:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's build the Autoencoder..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: instead of using the `fully_connected()` function from the `tensorflow.contrib.layers` module (as in the book), we now use the `dense()` function from the `tf.layers` module, which did not exist when this chapter was written. This is preferable because anything in contrib may change or be deleted without notice, while `tf.layers` is part of the official API. As you will see, the code is mostly the same.\n",
    "\n",
    "The main differences relevant to this chapter are:\n",
    "* the `scope` parameter was renamed to `name`, and the `_fn` suffix was removed in all the parameters that had it (for example the `activation_fn` parameter was renamed to `activation`).\n",
    "* the `weights` parameter was renamed to `kernel` and the weights variable is now named `\"kernel\"` rather than `\"weights\"`,\n",
    "* the bias variable is now named `\"bias\"` rather than `\"biases\"`,\n",
    "* the default activation is `None` instead of `tf.nn.relu`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "reset_graph()\n",
    "\n",
    "n_inputs = 3\n",
    "n_hidden = 2  # codings\n",
    "n_outputs = n_inputs\n",
    "\n",
    "learning_rate = 0.01\n",
    "\n",
    "X = tf.placeholder(tf.float32, shape=[None, n_inputs])\n",
    "hidden = tf.layers.dense(X, n_hidden)\n",
    "outputs = tf.layers.dense(hidden, n_outputs)\n",
    "\n",
    "reconstruction_loss = tf.reduce_mean(tf.square(outputs - X))\n",
    "\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate)\n",
    "training_op = optimizer.minimize(reconstruction_loss)\n",
    "\n",
    "init = tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_iterations = 1000\n",
    "codings = hidden\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    init.run()\n",
    "    for iteration in range(n_iterations):\n",
    "        training_op.run(feed_dict={X: X_train})\n",
    "    codings_val = codings.eval(feed_dict={X: X_test})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "与以前章节所构建的MLP相比，这段代码并没有多大的不同。需要注意的有两点：\n",
    "* 输出的数量等于输入的数量。\n",
    "* 为了执行简单的PCA，我们设置activation_fn=None（即，所有的神经元是线性的），并且成本函数是MSE。很快我们就会看到更加复杂的自动编码器。 \n",
    "\n",
    "现在让我们来加载数据集，在训练集上训练模型，并使用该模型来对测试集进行编码（即，将其投影为2D数据）："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving figure linear_autoencoder_pca_plot\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 288x216 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig = plt.figure(figsize=(4,3))\n",
    "plt.plot(codings_val[:,0], codings_val[:, 1], \"b.\")\n",
    "plt.xlabel(\"$z_1$\", fontsize=18)\n",
    "plt.ylabel(\"$z_2$\", fontsize=18, rotation=0)\n",
    "save_fig(\"linear_autoencoder_pca_plot\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "图15-2显示了原始的3D数据集（左侧）和自动编码器隐藏层的输出（即编码层，如图右侧所示）。可以看到，自动编码器找到了最好的2D平台来展现数据，尽可能多地保留数据的差异性（正如PCA一样）。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='images/autoencoders/PCA performed by an undercomplete linear autoencoder.png' >"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "图15-2：不完整的线性自动编码器实现的PCA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 栈式自动编码器 Stacked Autoencoders"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "与我们之前讨论的其他神经网络一样，自动编码器可以有多个隐藏层。在这种情况下，它被称为栈式自动编码器（或者深度自动编码器）。增加更多的层帮助自动编码器学习更加复杂的编码。然而，需要小心的是，不要让自动编码器变得太强大。想象一下，编码器强大到只是学习将每个输入映射到任意单个数字上（解码器学习翻转映射）。显然，这样的自动编码器将完美地重建训练数据，但是在这个过程中没有任何有效的数据表示（并且不可能推广到新的实例）。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "栈式自动编码器的结构通常对称于中央隐藏层（编码层）。为了简单起见，它看起来像一个三明治。例如，一个MNIST的自动编码器（见第3章）可能有784个输入，随后是一个有300个神经元的隐藏层，然后是一个有150个神经元的中央隐藏层，然后是一个有300个神经元的隐藏层，最后是一个有784个神经元的输出层。这个栈式自动编码器如图15-3所示。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='images\\autoencoders\\Stacked autoencoder.png' >"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TensorFlow实现"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's use MNIST:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-9-c50d5bb4a85c>:2: read_data_sets (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n",
      "WARNING:tensorflow:From c:\\users\\weis\\docume~1\\python scripts\\hands-on machine learning with scikit-learn tensorflow\\ml_path\\env\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:260: maybe_download (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please write your own downloading logic.\n",
      "WARNING:tensorflow:From c:\\users\\weis\\docume~1\\python scripts\\hands-on machine learning with scikit-learn tensorflow\\ml_path\\env\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:262: extract_images (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n",
      "Extracting /tmp/data/train-images-idx3-ubyte.gz\n",
      "WARNING:tensorflow:From c:\\users\\weis\\docume~1\\python scripts\\hands-on machine learning with scikit-learn tensorflow\\ml_path\\env\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:267: extract_labels (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n",
      "Extracting /tmp/data/train-labels-idx1-ubyte.gz\n",
      "Extracting /tmp/data/t10k-images-idx3-ubyte.gz\n",
      "Extracting /tmp/data/t10k-labels-idx1-ubyte.gz\n",
      "WARNING:tensorflow:From c:\\users\\weis\\docume~1\\python scripts\\hands-on machine learning with scikit-learn tensorflow\\ml_path\\env\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:290: DataSet.__init__ (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets(\"/tmp/data/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Train all layers at once"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's build a stacked Autoencoder with 3 hidden layers and 1 output layer (ie. 2 stacked Autoencoders). We will use ELU activation, He initialization and L2 regularization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "可以像实现一个常规的深度MLP一样实现一个栈式自动编码器。特别是，可以使用我们在第11章训练深度网络的技术实现。例如，以下代码使用He初始化，ELU激活函数，以及正则化构建了一个MNIST栈式自动编码器。除了没有标签外（没有y），代码看起来非常熟悉："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: since the `tf.layers.dense()` function is incompatible with `tf.contrib.layers.arg_scope()` (which is used in the book), we now use python's `functools.partial()` function instead. It makes it easy to create a `my_dense_layer()` function that just calls `tf.layers.dense()` with the desired parameters automatically set (unless they are overridden when calling `my_dense_layer()`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "reset_graph()\n",
    "\n",
    "from functools import partial\n",
    "\n",
    "n_inputs = 28 * 28\n",
    "n_hidden1 = 300\n",
    "n_hidden2 = 150  # codings\n",
    "n_hidden3 = n_hidden1\n",
    "n_outputs = n_inputs\n",
    "\n",
    "learning_rate = 0.01\n",
    "l2_reg = 0.0001\n",
    "\n",
    "X = tf.placeholder(tf.float32, shape=[None, n_inputs])\n",
    "\n",
    "he_init = tf.contrib.layers.variance_scaling_initializer() # He initialization\n",
    "#Equivalent to:\n",
    "#he_init = lambda shape, dtype=tf.float32: tf.truncated_normal(shape, 0., stddev=np.sqrt(2/shape[0]))\n",
    "l2_regularizer = tf.contrib.layers.l2_regularizer(l2_reg)\n",
    "my_dense_layer = partial(tf.layers.dense,\n",
    "                         activation=tf.nn.elu,\n",
    "                         kernel_initializer=he_init,\n",
    "                         kernel_regularizer=l2_regularizer)\n",
    "\n",
    "hidden1 = my_dense_layer(X, n_hidden1)\n",
    "hidden2 = my_dense_layer(hidden1, n_hidden2)\n",
    "hidden3 = my_dense_layer(hidden2, n_hidden3)\n",
    "outputs = my_dense_layer(hidden3, n_outputs, activation=None)\n",
    "\n",
    "reconstruction_loss = tf.reduce_mean(tf.square(outputs - X))\n",
    "\n",
    "reg_losses = tf.get_collection(tf.GraphKeys.REGULARIZATION_LOSSES)\n",
    "loss = tf.add_n([reconstruction_loss] + reg_losses)\n",
    "\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate)\n",
    "training_op = optimizer.minimize(loss)\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "saver = tf.train.Saver() # not shown in the book"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's train it! Note that we don't feed target values (`y_batch` is not used). This is unsupervised training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 Train MSE: 0.020329092\n",
      "1 Train MSE: 0.0114075765\n",
      "2 Train MSE: 0.01021888\n",
      "3 Train MSE: 0.009900079\n",
      "4 Train MSE: 0.010373402\n"
     ]
    }
   ],
   "source": [
    "n_epochs = 5\n",
    "batch_size = 150\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    init.run()\n",
    "    for epoch in range(n_epochs):\n",
    "        n_batches = mnist.train.num_examples // batch_size\n",
    "        for iteration in range(n_batches):\n",
    "            print(\"\\r{}%\".format(100 * iteration // n_batches), end=\"\") # not shown in the book\n",
    "            sys.stdout.flush()                                          # not shown\n",
    "            X_batch, y_batch = mnist.train.next_batch(batch_size)\n",
    "            sess.run(training_op, feed_dict={X: X_batch})\n",
    "        loss_train = reconstruction_loss.eval(feed_dict={X: X_batch})   # not shown\n",
    "        print(\"\\r{}\".format(epoch), \"Train MSE:\", loss_train)           # not shown\n",
    "        saver.save(sess, \"./my_model_all_layers.ckpt\")                  # not shown"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MNIST栈式自动编码器。除了没有标签外（没有y），代码看起来非常熟悉："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function loads the model, evaluates it on the test set (it measures the reconstruction error), then it displays the original image and its reconstruction:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_reconstructed_digits(X, outputs, model_path = None, n_test_digits = 2):\n",
    "    with tf.Session() as sess:\n",
    "        if model_path:\n",
    "            saver.restore(sess, model_path)\n",
    "        X_test = mnist.test.images[:n_test_digits]\n",
    "        outputs_val = outputs.eval(feed_dict={X: X_test})\n",
    "\n",
    "    fig = plt.figure(figsize=(8, 3 * n_test_digits))\n",
    "    for digit_index in range(n_test_digits):\n",
    "        plt.subplot(n_test_digits, 2, digit_index * 2 + 1)\n",
    "        plot_image(X_test[digit_index])\n",
    "        plt.subplot(n_test_digits, 2, digit_index * 2 + 2)\n",
    "        plot_image(outputs_val[digit_index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./my_model_all_layers.ckpt\n",
      "Saving figure reconstruction_ploat\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x432 with 4 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_reconstructed_digits(X, outputs, \"./my_model_all_layers.ckpt\")\n",
    "save_fig(\"reconstruction_ploat\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 权重绑定"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "当自动编码器和我们刚刚构建的那样严格对称时，一种常见的技术是将解码层的权重和编码层的权重联系起来。这种方式将模型的权重减半，提高训练速度，`限制了过度配置的风险`。具体而言，如果自动编码器总共有N层（输入层不计算在内），WL代码第L层的连接权重（例如，第一层是输入层，第N/2层是编码层，第N层是输出层），然后解码层的权重可以简单地定义为：WN-L+1= （其中，L=1，2，…N/2，）。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "不幸的是，使用TensorFlow的fully_connected（）函数实现权重绑定有点复杂；实际上，手动定义这些层更加容易一点。代码最终显得更加冗长："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is common to tie the weights of the encoder and the decoder (`weights_decoder = tf.transpose(weights_encoder)`). Unfortunately this makes it impossible (or very tricky) to use the `tf.layers.dense()` function, so we need to build the Autoencoder manually:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "reset_graph()\n",
    "\n",
    "n_inputs = 28 * 28\n",
    "n_hidden1 = 300\n",
    "n_hidden2 = 150  # codings\n",
    "n_hidden3 = n_hidden1\n",
    "n_outputs = n_inputs\n",
    "\n",
    "learning_rate = 0.01\n",
    "l2_reg = 0.0005"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "activation = tf.nn.elu\n",
    "regularizer = tf.contrib.layers.l2_regularizer(l2_reg)\n",
    "initializer = tf.contrib.layers.variance_scaling_initializer()\n",
    "\n",
    "X = tf.placeholder(tf.float32, shape=[None, n_inputs])\n",
    "\n",
    "weights1_init = initializer([n_inputs, n_hidden1])\n",
    "weights2_init = initializer([n_hidden1, n_hidden2])\n",
    "\n",
    "weights1 = tf.Variable(weights1_init, dtype=tf.float32, name=\"weights1\")\n",
    "weights2 = tf.Variable(weights2_init, dtype=tf.float32, name=\"weights2\")\n",
    "weights3 = tf.transpose(weights2, name=\"weights3\")  # tied weights\n",
    "weights4 = tf.transpose(weights1, name=\"weights4\")  # tied weights\n",
    "\n",
    "biases1 = tf.Variable(tf.zeros(n_hidden1), name=\"biases1\")\n",
    "biases2 = tf.Variable(tf.zeros(n_hidden2), name=\"biases2\")\n",
    "biases3 = tf.Variable(tf.zeros(n_hidden3), name=\"biases3\")\n",
    "biases4 = tf.Variable(tf.zeros(n_outputs), name=\"biases4\")\n",
    "\n",
    "hidden1 = activation(tf.matmul(X, weights1) + biases1)\n",
    "hidden2 = activation(tf.matmul(hidden1, weights2) + biases2)\n",
    "hidden3 = activation(tf.matmul(hidden2, weights3) + biases3)\n",
    "outputs = tf.matmul(hidden3, weights4) + biases4\n",
    "\n",
    "reconstruction_loss = tf.reduce_mean(tf.square(outputs - X))\n",
    "reg_loss = regularizer(weights1) + regularizer(weights2)\n",
    "loss = reconstruction_loss + reg_loss\n",
    "\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate)\n",
    "training_op = optimizer.minimize(loss)\n",
    "\n",
    "init = tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这些代码很简单，但是有几个重要事情需要注意：\n",
    "* 第一，weights3和weights4不是变量，它们分别是weights2和weights1的转置（它们被“绑定”在一起）。\n",
    "* 第二，因为它们不是变量，所以没有必要进行正则化：只正则化weights1和weights2。\n",
    "* 第三，偏置项从来不会被绑定，也不会被正则化。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 一次训练一个自动编码器"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "相比于像我们这样一次训练整个栈式自动编码器，一次训练一个单独的自动编码器会快很多，然后将它们堆叠为一个栈式自动编码器（因此而得名），如图15-4所示。这种方法对深层自动编码器尤其有用。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='images/autoencoders/Training one autoencoder at a time.png' >"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "图15-4：一次训练一个自动编码器"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "reset_graph()\n",
    "\n",
    "from functools import partial\n",
    "\n",
    "def train_autoencoder(X_train, n_neurons, n_epochs, batch_size,\n",
    "                      learning_rate = 0.01, l2_reg = 0.0005, seed=42,\n",
    "                      hidden_activation=tf.nn.elu,\n",
    "                      output_activation=tf.nn.elu):\n",
    "    graph = tf.Graph()\n",
    "    with graph.as_default():\n",
    "        tf.set_random_seed(seed)\n",
    "\n",
    "        n_inputs = X_train.shape[1]\n",
    "\n",
    "        X = tf.placeholder(tf.float32, shape=[None, n_inputs])\n",
    "        \n",
    "        my_dense_layer = partial(\n",
    "            tf.layers.dense,\n",
    "            kernel_initializer=tf.contrib.layers.variance_scaling_initializer(),\n",
    "            kernel_regularizer=tf.contrib.layers.l2_regularizer(l2_reg))\n",
    "\n",
    "        hidden = my_dense_layer(X, n_neurons, activation=hidden_activation, name=\"hidden\")\n",
    "        outputs = my_dense_layer(hidden, n_inputs, activation=output_activation, name=\"outputs\")\n",
    "\n",
    "        reconstruction_loss = tf.reduce_mean(tf.square(outputs - X))\n",
    "\n",
    "        reg_losses = tf.get_collection(tf.GraphKeys.REGULARIZATION_LOSSES)\n",
    "        loss = tf.add_n([reconstruction_loss] + reg_losses)\n",
    "\n",
    "        optimizer = tf.train.AdamOptimizer(learning_rate)\n",
    "        training_op = optimizer.minimize(loss)\n",
    "\n",
    "        init = tf.global_variables_initializer()\n",
    "\n",
    "    with tf.Session(graph=graph) as sess:\n",
    "        init.run()\n",
    "        for epoch in range(n_epochs):\n",
    "            n_batches = len(X_train) // batch_size\n",
    "            for iteration in range(n_batches):\n",
    "                print(\"\\r{}%\".format(100 * iteration // n_batches), end=\"\")\n",
    "                sys.stdout.flush()\n",
    "                indices = rnd.permutation(len(X_train))[:batch_size]\n",
    "                X_batch = X_train[indices]\n",
    "                sess.run(training_op, feed_dict={X: X_batch})\n",
    "            loss_train = reconstruction_loss.eval(feed_dict={X: X_batch})\n",
    "            print(\"\\r{}\".format(epoch), \"Train MSE:\", loss_train)\n",
    "        params = dict([(var.name, var.eval()) for var in tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES)])\n",
    "        hidden_val = hidden.eval(feed_dict={X: X_train})\n",
    "        return hidden_val, params[\"hidden/kernel:0\"], params[\"hidden/bias:0\"], params[\"outputs/kernel:0\"], params[\"outputs/bias:0\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 Train MSE: 0.018551283\n",
      "1 Train MSE: 0.018578433\n",
      "2 Train MSE: 0.019342232\n",
      "39% Train MSE: 0.019147895\n",
      "0 Train MSE: 0.004186158\n",
      "1 Train MSE: 0.00433809\n",
      "2 Train MSE: 0.0046690544\n",
      "3 Train MSE: 0.004427336\n"
     ]
    }
   ],
   "source": [
    "hidden_output, W1, b1, W4, b4 = train_autoencoder(mnist.train.images, n_neurons=300, n_epochs=4, batch_size=150,\n",
    "                                                  output_activation=None)\n",
    "_, W2, b2, W3, b3 = train_autoencoder(hidden_output, n_neurons=150, n_epochs=4, batch_size=150)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we can create a Stacked Autoencoder by simply reusing the weights and biases from the Autoencoders we just trained:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "reset_graph()\n",
    "\n",
    "n_inputs = 28*28\n",
    "\n",
    "X = tf.placeholder(tf.float32, shape=[None, n_inputs])\n",
    "hidden1 = tf.nn.elu(tf.matmul(X, W1) + b1)\n",
    "hidden2 = tf.nn.elu(tf.matmul(hidden1, W2) + b2)\n",
    "hidden3 = tf.nn.elu(tf.matmul(hidden2, W3) + b3)\n",
    "outputs = tf.matmul(hidden3, W4) + b4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x432 with 4 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_reconstructed_digits(X, outputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在训练的第一阶段，第一个自动编码器学习重现输入。在第二阶段，第二个自动编码器学习重建第一个自动编码器隐藏层的输出。最终，用这些所有的自动编码器构建了一个大三明治，如图15-4所示（即，首先堆叠各个自动编码器的隐藏层，然后输出层将其顺序反转）。这样就得到了最终的栈式自动编码器。用这种方式，可以轻松地训练更多的自动编码器，来构建一个非常深层的栈式自动编码器。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "为了实现这种多阶段训练算法，最简单的方法是对每个阶段使用不同的TensorFlow图，只需通过它来运行训练集并捕获隐藏层的输出。该输出随后作为下一个自动编码器的训练集。一旦用这种方式对所有的自动编码器都进行了训练，只需复制每个自动编码器的权重和偏置系数，并使用它们构建栈式自动编码器。实现方式非常简单，不在这里赘述，请查看Jupyter笔记本里面的代码（https://github.com/ageron/handson-ml） 作为实例。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "另外一种途径是使用一个包含整个栈式自动编码器的单个图表，再加上一些为了执行每个训练阶段的额外操作，如图15-5所示。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='images/autoencoders/A single graph to train a stacked autoencoder.png' >"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "图15-5：训练栈式自动编码器的单个图表"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这里需要一点解释：\n",
    "* 图表的中间列是完全栈式自动编码器。这部分可以在训练后使用。\n",
    "* 左列是第一阶段训练需要的一系列操作。它创造了一个绕过第二个和第三个隐藏层的输出层。这个输出层和栈式自动编码器**分享相同的权重和偏差系数**。在其之上是训练操作，旨在使得输出尽可能地接近输入。因此，该阶段将为第一个隐藏层和输出层（即第一个自动编码器）训练权重和偏差系数。\n",
    "* 右列是第二阶段训练需要的一系列操作。其训练操作旨在使得第二个隐藏层的输出尽可能与第一个隐藏层的输出相似。注意，**必须在运行第二阶段时冻结第一隐藏层**。这个阶段将训练第二个和第三个隐藏层（即第二个自动编码器）的权重和偏差系数。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate)\n",
    "\n",
    "with tf.name_scope(\"phase1\"):\n",
    "    phase1_outputs = tf.matmul(hidden1, weights4) + biases4  # bypass hidden2 and hidden3\n",
    "    phase1_reconstruction_loss = tf.reduce_mean(tf.square(phase1_outputs - X))\n",
    "    phase1_reg_loss = regularizer(weights1) + regularizer(weights4)\n",
    "    phase1_loss = phase1_reconstruction_loss + phase1_reg_loss\n",
    "    phase1_training_op = optimizer.minimize(phase1_loss)\n",
    "\n",
    "with tf.name_scope(\"phase2\"):\n",
    "    phase2_reconstruction_loss = tf.reduce_mean(tf.square(hidden3 - hidden1))\n",
    "    phase2_reg_loss = regularizer(weights2) + regularizer(weights3)\n",
    "    phase2_loss = phase2_reconstruction_loss + phase2_reg_loss\n",
    "    train_vars = [weights2, biases2, weights3, biases3]\n",
    "    phase2_training_op = optimizer.minimize(phase2_loss, var_list=train_vars) # freeze hidden1\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "第一阶段很简单：创造了一个忽略第二个和第三个隐藏层的输出层，然后构建训练操作来最小化输入和输出之间的距离（加上一些正则化）。 \n",
    "\n",
    "第二阶段增加了最小化隐藏层3和隐藏层1输出之间的距离所需要的操作（也有一些正则化）。更重要的是，我们为minimize（）方法提供了可训练的变量列表，确保忽略weights1和biases1；以便在第二阶段有效地冻结隐藏层1。  \n",
    "\n",
    "在执行阶段，需要做的所有事情是在第一阶段训练操作数次，然后执行第二阶段训练操作更多次。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*因为在第二阶段，隐藏层1被冻结，所以对于任意给定的训练实例它的输出总是相同的。为了避免需要在每个单独的时间点重新计算隐藏层1的输出，可以在第一阶段结束的时候为整个训练集计算它，然后直接在第二阶段馈送隐藏层1的输出缓存。这将有很多的性能提升。*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training one Autoencoder at a time in a single graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "reset_graph()\n",
    "\n",
    "n_inputs = 28 * 28\n",
    "n_hidden1 = 300\n",
    "n_hidden2 = 150  # codings\n",
    "n_hidden3 = n_hidden1\n",
    "n_outputs = n_inputs\n",
    "\n",
    "learning_rate = 0.01\n",
    "l2_reg = 0.0001\n",
    "\n",
    "activation = tf.nn.elu\n",
    "regularizer = tf.contrib.layers.l2_regularizer(l2_reg)\n",
    "initializer = tf.contrib.layers.variance_scaling_initializer()\n",
    "\n",
    "X = tf.placeholder(tf.float32, shape=[None, n_inputs])\n",
    "\n",
    "weights1_init = initializer([n_inputs, n_hidden1])\n",
    "weights2_init = initializer([n_hidden1, n_hidden2])\n",
    "weights3_init = initializer([n_hidden2, n_hidden3])\n",
    "weights4_init = initializer([n_hidden3, n_outputs])\n",
    "\n",
    "weights1 = tf.Variable(weights1_init, dtype=tf.float32, name=\"weights1\")\n",
    "weights2 = tf.Variable(weights2_init, dtype=tf.float32, name=\"weights2\")\n",
    "weights3 = tf.Variable(weights3_init, dtype=tf.float32, name=\"weights3\")\n",
    "weights4 = tf.Variable(weights4_init, dtype=tf.float32, name=\"weights4\")\n",
    "\n",
    "biases1 = tf.Variable(tf.zeros(n_hidden1), name=\"biases1\")\n",
    "biases2 = tf.Variable(tf.zeros(n_hidden2), name=\"biases2\")\n",
    "biases3 = tf.Variable(tf.zeros(n_hidden3), name=\"biases3\")\n",
    "biases4 = tf.Variable(tf.zeros(n_outputs), name=\"biases4\")\n",
    "\n",
    "hidden1 = activation(tf.matmul(X, weights1) + biases1)\n",
    "hidden2 = activation(tf.matmul(hidden1, weights2) + biases2)\n",
    "hidden3 = activation(tf.matmul(hidden2, weights3) + biases3)\n",
    "outputs = tf.matmul(hidden3, weights4) + biases4\n",
    "\n",
    "reconstruction_loss = tf.reduce_mean(tf.square(outputs - X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.train.AdamOptimizer(learning_rate)\n",
    "\n",
    "with tf.name_scope(\"phase1\"):\n",
    "    phase1_outputs = tf.matmul(hidden1, weights4) + biases4  # bypass hidden2 and hidden3\n",
    "    phase1_reconstruction_loss = tf.reduce_mean(tf.square(phase1_outputs - X))\n",
    "    phase1_reg_loss = regularizer(weights1) + regularizer(weights4)\n",
    "    phase1_loss = phase1_reconstruction_loss + phase1_reg_loss\n",
    "    phase1_training_op = optimizer.minimize(phase1_loss)\n",
    "\n",
    "with tf.name_scope(\"phase2\"):\n",
    "    phase2_reconstruction_loss = tf.reduce_mean(tf.square(hidden3 - hidden1))\n",
    "    phase2_reg_loss = regularizer(weights2) + regularizer(weights3)\n",
    "    phase2_loss = phase2_reconstruction_loss + phase2_reg_loss\n",
    "    train_vars = [weights2, biases2, weights3, biases3]\n",
    "    phase2_training_op = optimizer.minimize(phase2_loss, var_list=train_vars) # freeze hidden1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "init = tf.global_variables_initializer()\n",
    "saver = tf.train.Saver()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training phase #1\n",
      "0 Train MSE: 0.008275491\n",
      "1 Train MSE: 0.007927759\n",
      "2 Train MSE: 0.0077320114\n",
      "3 Train MSE: 0.007602796\n",
      "Training phase #2\n",
      "0 Train MSE: 0.19699986\n",
      "1 Train MSE: 0.0059469314\n",
      "2 Train MSE: 0.0026474535\n",
      "3 Train MSE: 0.0020872727\n",
      "Test MSE: 0.009720687\n"
     ]
    }
   ],
   "source": [
    "training_ops = [phase1_training_op, phase2_training_op]\n",
    "reconstruction_losses = [phase1_reconstruction_loss, phase2_reconstruction_loss]\n",
    "n_epochs = [4, 4]\n",
    "batch_sizes = [150, 150]\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    init.run()\n",
    "    for phase in range(2):\n",
    "        print(\"Training phase #{}\".format(phase + 1))\n",
    "        for epoch in range(n_epochs[phase]):\n",
    "            n_batches = mnist.train.num_examples // batch_sizes[phase]\n",
    "            for iteration in range(n_batches):\n",
    "                print(\"\\r{}%\".format(100 * iteration // n_batches), end=\"\")\n",
    "                sys.stdout.flush()\n",
    "                X_batch, y_batch = mnist.train.next_batch(batch_sizes[phase])\n",
    "                sess.run(training_ops[phase], feed_dict={X: X_batch})\n",
    "            loss_train = reconstruction_losses[phase].eval(feed_dict={X: X_batch})\n",
    "            print(\"\\r{}\".format(epoch), \"Train MSE:\", loss_train)\n",
    "            saver.save(sess, \"./my_model_one_at_a_time.ckpt\")\n",
    "    loss_test = reconstruction_loss.eval(feed_dict={X: mnist.test.images})\n",
    "    print(\"Test MSE:\", loss_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 重建可视化"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "保证自动编码器被合适训练的一种方法是比较输入和输出。它们必须相当相似，差异应该是一些不重要的细节。我们来绘制两个随机数字及其重建："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cache the frozen layer outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training phase #1\n",
      "0 %Train MSE: 0.008117558\n",
      "1 Train MSE: 0.007595798\n",
      "2 Train MSE: 0.007454363\n",
      "3 Train MSE: 0.0077533526\n",
      "Training phase #2\n",
      "0 Train MSE: 0.078076646\n",
      "1 Train MSE: 0.004299964\n",
      "2 Train MSE: 0.0023144884\n",
      "3 Train MSE: 0.0019754742\n",
      "Test MSE: 0.009669314\n"
     ]
    }
   ],
   "source": [
    "training_ops = [phase1_training_op, phase2_training_op]\n",
    "reconstruction_losses = [phase1_reconstruction_loss, phase2_reconstruction_loss]\n",
    "n_epochs = [4, 4]\n",
    "batch_sizes = [150, 150]\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    init.run()\n",
    "    for phase in range(2):\n",
    "        print(\"Training phase #{}\".format(phase + 1))\n",
    "        if phase == 1:\n",
    "            hidden1_cache = hidden1.eval(feed_dict={X: mnist.train.images})\n",
    "        for epoch in range(n_epochs[phase]):\n",
    "            n_batches = mnist.train.num_examples // batch_sizes[phase]\n",
    "            for iteration in range(n_batches):\n",
    "                print(\"\\r{}%\".format(100 * iteration // n_batches), end=\"\")\n",
    "                sys.stdout.flush()\n",
    "                if phase == 1:\n",
    "                    indices = rnd.permutation(mnist.train.num_examples)\n",
    "                    hidden1_batch = hidden1_cache[indices[:batch_sizes[phase]]]\n",
    "                    feed_dict = {hidden1: hidden1_batch}\n",
    "                    sess.run(training_ops[phase], feed_dict=feed_dict)\n",
    "                else:\n",
    "                    X_batch, y_batch = mnist.train.next_batch(batch_sizes[phase])\n",
    "                    feed_dict = {X: X_batch}\n",
    "                    sess.run(training_ops[phase], feed_dict=feed_dict)\n",
    "            loss_train = reconstruction_losses[phase].eval(feed_dict=feed_dict)\n",
    "            print(\"\\r{}\".format(epoch), \"Train MSE:\", loss_train)\n",
    "            saver.save(sess, \"./my_model_cache_frozen.ckpt\")\n",
    "    loss_test = reconstruction_loss.eval(feed_dict={X: mnist.test.images})\n",
    "    print(\"Test MSE:\", loss_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAawAAAFqCAYAAABGeW4FAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4yLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvOIA7rQAAGwxJREFUeJzt3VtsVXXax/E/Aj2XciqlVARFVGyi8RRPM9F44c1EjRov1AuNmmhMNNFEjSYm6o3eOdwYncxEZ5y5mMRkZjLGQ9SIilE8RA6KKCIIldKCUGrPFHxvfN+Zt8/vkf9y793dp/1+Lp+s/1qrG3cfV9avz3/WTz/9lAAAmOpOqPYNAACQg4YFAAiBhgUACIGGBQAIgYYFAAiBhgUACIGGBQAIgYYFAAhhTpWuy18ro9xmVfsGZqru7m6+zyir9vZ2+X3mCQsAEAINCwAQAg0LABBCtd5hAcCUMGvW5Lz+9AaNT9b1Peq+1D1NhUHpPGEBAEKgYQEAQqBhAQBCoGEBAEKgYQEAQiAlCCC0aqfsZs+ebWpFUnZHjx7Nus4JJ+jnC7W+yGeSe2yRc1YqUcgTFgAgBBoWACAEGhYAIAQaFgAgBEIXAKqqSEAh99hSgxgqSJFSfkBiZGQke6261tjYWPY91dXVZd1Tkc/k2LFj2cdOJp6wAAAh0LAAACHQsAAAIdCwAAAhELoAMOV4AQEVPBgfHzc1L7TR0NCQdezo6Gj29VWYQh2nrp2SDmgUodbPmWN/tXuhjblz52at90IjuZM2yjH9gicsAEAINCwAQAg0LABACDQsAEAINCwAQAikBAFMOd5ooNzRSF7KUCXq1LHedQYGBkxN7VOlEoFeSnDhwoVZ1/eSi2qMkzrWG+GUm3xUacyUdPqvSCKwyMgonrAAACHQsAAAIdCwAAAh0LAAACEQugAwaXIDDt6LeBUwUMd661VoQl1fXSclHTyYN2+eqamAgxdEOHjwYNY9qRFK3rFHjhwxNS80ocYwqWt5n6kKnaiauqeieMICAIRAwwIAhEDDAgCEQMMCAIQwI0MXH374oamtXbtWHtvR0WFq9fX1pnbLLbfI9eqv2FUNmE68F/RqgoUKI/T19WWf98CBA6Z2+PBhub6/vz/rnrw9qtR9nXjiiVn3qcIVKaW0f//+rOt7kzKamppMbdGiRaamfpelpH8ftbS0mFpzc7NcrwIWlcITFgAgBBoWACAEGhYAIAQaFgAgBBoWACCEWUX2LSmjqlz0f51++ummtn379opcS6VtLrrooopcq9xWrlwp6w8//LCpnXTSSRW+m+PK31QHZdXd3Z39fVbjgVRKT41QSimlnp4eU/v+++9Nbffu3XL9rl27sq6vzpmSTsQNDw+b2p49e0xt79698pxq/fz5801twYIFcr0azdTa2mpql19+uVx/2WWXmdqaNWtMTaURU9KJSPXvrPbd8ixdulR+n3nCAgCEQMMCAIRAwwIAhEDDAgCEMCNHM/3zn/80tY0bN8pjOzs7Te2LL74wtQ0bNsj1//rXv0zt9ddfN7WTTz7Z1Hbu3CnPmUvtc5NSSu3t7aamXhJ7VBjjoYceyl4P/DcVOvBGK6kxRr29vVm1lFIaGhoyNbVPU21trVyvRjv98MMPWedU37uU9H5aixcvNjVvNNPmzZtNTf2OUmGzlPTIKBVEGR0dletz987yAn7eGC+FJywAQAg0LABACDQsAEAINCwAQAgzMnSh/opb1TxnnXWWqd14443y2KeeesrU1F/bq9DFt99+m31PSk1Njayrl7/q+uoFd0opnXHGGSXdF6aXIi/N1ct8NQFBHZeSnhyjpl8sX748e726fxUkSCmlQ4cOyfpEalKFF+RQkyqKTP9QUznUVI3u7m65Xv2sKrDl/T5Rn5+qqZ/TO9bDExYAIAQaFgAgBBoWACAEGhYAIAQaFgAghBmZEpxMdXV1ppabsiuSXCxCjZE6cOCAqV144YVy/ZVXXln2e0JcRfbUU/tJKd7eT+r7NDIyYmpe8kxdX+0d5aUU1WgnNVpJ8UYrqfSdSiN+9dVXcv2WLVuyjm1ubpbrGxsbTU0lN73PVKX/1Hrv377Qfz/ZRwIAUEU0LABACDQsAEAINCwAQAiELqaxwcFBWb/22mtNTb1k/v3vfy/X19fXl3ZjmPa8F/TqZbwaDeS9oFfnXbVqlal5Y4DUf7sqjODtJaf27lIjk9R1vO+N+llVkMQblbZjxw5TU0GGjo4OuX7p0qWm5o2mUoqEJkrFExYAIAQaFgAgBBoWACAEGhYAIARCF9PYCy+8IOv79u0ztUWLFpnaihUryn1LmCG8F/EqYKD2iRofHy/p+t7eTaquat79q9CImsqhJnJ4QQYVjhodHTW1nTt3yvVqPzA1TefMM8+U6xcuXGhq6jMpMqlChWPKEc7gCQsAEAINCwAQAg0LABACDQsAEAKhi2lC/bX7/fffn73+gw8+MDX1F/BAKXJf0KtwQ0optbS0mJoKaHjbgyjqWDVpIqWUfvzxR1NT23Oo9WptSnorkc2bN5vau+++K9erz0+FLs4//3y5fsmSJaamPpMi00cUQhcAgBmDhgUACIGGBQAIgYYFAAiBhgUACIGU4DTx73//29SOHDkij73hhhtM7ZRTTin7PWFmKDKGJzdR5qUE1Xm99JqiRh719/ebWl9fn1yv0nNq5JJKLqprp5TSpk2bTO3ll182te7ubrm+s7PT1FQiUKUBU9Kfn/p3UnuBeXLToEXxhAUACIGGBQAIgYYFAAiBhgUACIHQRUAqTPGPf/zD1NQ+Qyml9OSTT5qa95IbqLQiY4DUy/zm5mZTGxsbk+uHhoZMTYUh5s+fL9er4IC6vrpPtQ9dSjp08dZbb2WdMyUdurjgggtMbfHixXK9+qzV53T06FG5Xv3uUOf09jgrEsbgCQsAEAINCwAQAg0LABACDQsAEAKhi4D+9Kc/mdp7771najfddJNcz1QLlFORqQal7odV6rQEdf05c+yvQS+wtHDhQlOrqakxtf3795va559/Ls/5xhtvmJoKopx77rly/TnnnGNqHR0dpuZ9pupaKmBR5LMvsvdVkWN5wgIAhEDDAgCEQMMCAIRAwwIAhEDDAgCEQEpwCtu4caOs33PPPaamRsk88cQTZb8nIEeR5JdKrxVJtI2MjJiat/eUWt/Q0GBqTU1Ncr1KD6rrb9myxdReeeUVec6uri5TW7NmjaldccUVcr3a+0qlGb1xVwMDA6amUoLev0mR/chKxRMWACAEGhYAIAQaFgAgBBoWACAEQhdTxPDwsKndeOON8lj1QvTmm282NUYwYapR431UEMILbai62h/Oo8YwKXPnzpV1da9qn6v169eb2tatW+U5VWDqkksuMbWrrrpKrl+xYoWpqfv3gije3mETlToWqxx4wgIAhEDDAgCEQMMCAIRAwwIAhEDoogrUi9vf/e53pvbVV1/J9eqv4B9//PHSbwwok1Jf0HvTE1TAQoWQvNBEc3OzqakJDt79Dw4OmtrmzZtN7YsvvjC13t5eeU61n5Xa+6q9vV2ub2xsNLXx8XFT80IX6vNT4RRCFwAAZKJhAQBCoGEBAEKgYQEAQqBhAQBCICVYBQcPHjS1devWZa9/8cUXTU3tfwNUizdaSSXNVErPS7TljmHyRjCpRJw6p0rZpZTStm3bTO3tt9/OOs67p1WrVplaZ2enqbW0tMj16jNVaUb1s6fk73M1kfdvqj6/InucFdk7jScsAEAINCwAQAg0LABACDQsAEAIhC4q7PDhw6Z20UUXZa3961//KutqlAsQQe5+Vt4YILVeBSSGhobkehV8UOu7urrk+k8++cTUtmzZYmq7d+82tdWrV8tznn/++abW1tZmat64KbWflfqcvH2vVBhC/Zt4oRE1Rkv9+xUJ4nh4wgIAhEDDAgCEQMMCAIRAwwIAhEDoosKef/55U/v222+z1v7mN7+R9amwLw1QLuqlvdozLqWUamtrs9Z7EzHUd6evry97/aFDh0xNTZBQ0ysuvvhiec6zzjrL1BoaGkxteHhYrldTLVSQxPtMVRjCC1go6jMt8juKSRcAgGmHhgUACIGGBQAIgYYFAAiBhgUACIGUYJls375d1h977LHJvRFgGlDJv5R0+kwl2ry9n1SiTiX/vv76a7l+//79pqbGKC1fvtzUli1bJs+5YMECU1NjlPr7++V69Vmpnz9336uU9Odc5N9EJf/KkW7mCQsAEAINCwAQAg0LABACDQsAEAKhizJ57733ZN17UTrRmjVrTK2+vr6kewKmmtwX794YIS9MkXsddV417qmmpkauV9/JefPmmVpHR4epdXZ2ynOqMUpF9qNS9z86OiqPzZW7x9Uv1SuBJywAQAg0LABACDQsAEAINCwAQAiELqrgkksuMbU33njD1AhdYLrJ3fvIm6qQO0HBu44KUzQ1NZmamlSRkt6namBgwNRUQKK9vV2es7Gx0dRUuMQLXaipGOrn9AIr3medq8h+VqXiCQsAEAINCwAQAg0LABACDQsAEAINCwAQwqzJTHj8l6pcFNPa5M2Hwf/T3d09ad/nUscAqfVqXJM3Gmp4eNjU5s6dm1Xzftfm/kyVSk5ORe3t7fJD4QkLABACDQsAEAINCwAQAg0LABBCtUIXAAAUwhMWACAEGhYAIAQaFgAgBBoWACAEGhYAIAQaFgAgBBoWACAEGhYAIAQaFgAgBBoWACAEGhYAIAQaFgAgBBoWACAEGhYAIAQaFgAgBBoWACAEGhYAIAQaFgAgBBoWACAEGhYAIAQaFgAgBBoWACAEGhYAIAQaFgAgBBoWACAEGhYAIAQaFgAgBBoWACAEGhYAIIQ5VbruT1W6LqavWdW+gZmqp6eH7zPKqq2tTX6fecICAIRAwwIAhEDDAgCEUK13WAAwZc2apV+JevWJfvrJvtZTNa+urpN7be9Y7/qlqtR5FZ6wAAAh0LAAACHQsAAAIdCwAAAh0LAAACGQEgQwY5xwgv1/9CNHjpial3xT68fHx03t6NGj2efMTQnOnTtXrj927JisTzRnjv51r36m2bNnZ53TU6nkIE9YAIAQaFgAgBBoWACAEGhYAIAQCF2Uyd/+9jdZHxwcNLVPP/3U1P7whz9kX+vRRx81tSuuuMLULr/88uxzAtVS6hihIseOjo5m1YaHh7PXj4yMmJoKXXhBhtraWlNTQQoVjkhJhzEaGhpMzQtdKLlBEK9O6AIAMKPRsAAAIdCwAAAh0LAAACHMmsy9TP5LVS5aLnfffbepPffcc1W4k/8488wzTW39+vXy2JaWlkrfTjXkbxaEsurp6cn+Ppeyn1RK+RMkvOkPhw4dMjUVmti7d69cv2PHDlPr6ur61feZkv5MGhsbTe3UU0+V61euXGlqra2tptbU1CTXq9CGN1VD8cIgudTn0tbWJv9D4QkLABACDQsAEAINCwAQAg0LABACDQsAEAKjmY6jEonAc845x9Suv/56U9u+fbtc/+c//9nUtm7damovvfSSXH/77bcf7xaBishNJRcZA6T2s/JGK6lE3zfffGNqO3fulOvVsQMDA6a2ePFiU6urq5PnVJ/Jjz/+aGpeunf16tWmVlNTY2reaCb1maoRVEX241LJwdyE6C/hCQsAEAINCwAQAg0LABACDQsAEAKhi5/t3r1b1v/4xz9mrb/gggtk/bXXXjM1tVeNekmqxrukpF/8vv/++6Z24MABuR6olty9k7wX9OoFv9pzzvs+f/3111nrx8bG5Ho1Aq25udnUVOhiaGhInlOFq3L37UpJf36LFi0yNW8/rvHxcVNTQZIi+2mpIIz6HVcUT1gAgBBoWACAEGhYAIAQaFgAgBAIXfzMCyioF5oqYPHmm2/K9d4eNDleeOEFWf/444+z1l9zzTW/+tpAJeQGLLz9rFRwQE1V8AICKvCk/Pa3v5X1M844w9RUwKG3t9fU3nnnnaxrp5RSbW1tVi0lHRBRn5/3u0iFLg4ePGhqRfbIUtcvsh+YhycsAEAINCwAQAg0LABACDQsAEAINCwAQAikBH927rnnyrpKD6oRI/X19WW/J28slDc2BqiG3HFL3rFF1quRP4o3Bqitrc3Uli5damqnnXaaXL9gwQJTU6Od9u7da2o9PT3ynGrvK3VPagSUd311Tu8zUSPg5s+fb2re753c5GY58IQFAAiBhgUACIGGBQAIgYYFAAiB0MVxtLS0TMp1XnzxRVPbtGlT9vorr7zS1FatWlXSPQE5vICEkjuGx3tpr65VV1dnat4Yovb2dlNrbW01tcbGxuPd4v9RYYoNGzaYmtrHLiV9r+pnWrhwoVyvgiDDw8Om5u2npUY+qXBLkXFZRcYtFcETFgAgBBoWACAEGhYAIAQaFgAgBEIXVfDZZ5+Z2p133mlq3ktS9eJ47dq1plZk/xqgnLwghpqq4O1dpahjVUBg+fLlcr0KGOSeMyW9T5T6Pn/++edZa1PS3+cLL7zQ1JYsWSLXz5s3z9RU6GFkZESuVxM01L9ToX2rRGiG/bAAADMGDQsAEAINCwAQAg0LABACDQsAEAIpwSr44IMPTM1LBCp33XWXqXn79wBTiUqKqZq3d5M6ViXa1Ggj71i1d5SXqNu4cWNWbc+ePaam9phKSaf0TjrpJFNTe3mlpEcj9ff3Zx2Xkv7do1J+XppvfHw8+1pKkdFePGEBAEKgYQEAQqBhAQBCoGEBAEIgdFFht912m6n9/e9/z1p73333yfqDDz5Y0j0BlVZk3I46dmxsLHu92ifKe+mvXvAfPnzY1Ly96FTAQo1mUtdX+1allNKKFStMTQUs1FiplIoFtpShoSFTy913LKXK7X2l8IQFAAiBhgUACIGGBQAIgYYFAAiB0EWZDAwMyPqrr75qauqv6NVL1kceeUSe05sCAEx1ufskedMPVJihyHoVEFB7X3333Xdy/bZt22R9IhVQOHbsmDxWhTEGBwdNzfsdo0IX6lrqs/fWq8/Z219PTQ9Rx3rhDCZdAACmHRoWACAEGhYAIAQaFgAgBEIXZXLDDTfIem9vb9b6e++919TUX/ADERR5wa6O9aY6qDBAbpAjJT3VQgUsurq65Hp1r2p7EHX/S5culedUP5Pa8kRt45GSH4aYSIUjUkqpvr7e1Pr6+kzNC22oEJgKsnjri0zK4AkLABACDQsAEAINCwAQAg0LABACDQsAEAIpwV/h008/NbV169Zlr7/uuutM7f777y/lloCwVHrMS5TlHjs8PCzX79ixw9Q+/PBDU9u/f79c39DQYGonn3yyqank3Zo1a+Q5TzzxRFObM8f+avb2+FI/vzrWGw2lRsWpmrfv1qJFi0xN/fzlwBMWACAEGhYAIAQaFgAgBBoWACAEQhfHoV7ePvzww6Y2NjaWfc7zzjvP1NjjCtNJkT2O1Ggeb1yPChioMUD79u2T67dv325qu3btMjVvDJIar6S+z62trVlrU9IBC/X7xBvztmzZMlNTAQl1nZT07zj1mbS3t2evJ3QBAJjRaFgAgBBoWACAEGhYAIAQCF0cx7PPPmtqb731Vvb62267zdSYaoGZSoUp1D5NXkBATXBQAYO9e/fK9d3d3Vnr1R5XKempFi0tLVn36VH7WQ0NDZmaF2RQoQc11cKb/vH999+b2sGDB03N+0yWLFliakWmlxQJ6PCEBQAIgYYFAAiBhgUACIGGBQAIgYYFAAiBlOBxPPLIIyWtf/rpp02NMUzAf6hEoJccU4lClWjr6uqS6/fs2WNqKiW3evVquV4l3VQi8fTTTzc172fq6+sztf7+flP74Ycf5Hr1+0Rd69ChQ3L97t27TU2lHFWa0TtWpRS9lGARPGEBAEKgYQEAQqBhAQBCoGEBAEIgdFFhAwMDplaOl48T1dbWmpo3Hka9uFbjaTxqxMvatWuz1yvqXr3Ai/fyF1OHt5+VCgOoF/SqlpLep0od6/33rL6PO3bsMDX1HUkppeXLl5taY2OjqX355Zem5n0fVcBC7YflfaZNTU2mpsIl3n5a6vu8atUqU/PGZXn3NVGREUwenrAAACHQsAAAIdCwAAAh0LAAACEQuqiwjo6OSbnOXXfdZWrLli2Tx+7bt8/UnnnmmbLfU6m8z+6OO+6Y5DtBueS+oPd4YYiJ6urqZF0FdubNm2dqKrSQUkrr1683tY8++sjUVEDBCy2oMILaY2r+/PlyvQqdqCCJF3pYsWKFqZ166qmmtmDBArlefaaVCJalxBMWACAIGhYAIAQaFgAgBBoWACAEGhYAIARSgsdx8803m9rzzz9fhTv5Zc8++2zZz+mlmrwRMxPdeuutsn7xxRdnrb/00kuzjsPU4yXSVEqwyGgllUirr683NbUfVUp6Tyl1Txs2bJDrv/nmG1NT+3ENDg5mXSclfa/qZ1IjoFJKqaenR9YnWrlypayfffbZpqZSgl7y0hujVQk8YQEAQqBhAQBCoGEBAEKgYQEAQphVjj1KfoWqXLRc/vKXv5ia2r+miE2bNplaqeOSHnjgAVlXL1SVq6++WtbV2JgpoLSZP/jVenp6sr/PuaEL70W+CgKpc6p9s1JK6cCBA6amQgtqBFNKKe3cudPUurq6TK27u9vUGhoa5Dk7OztNrbW11dRUECMlHcZoaWkxNbWXV0o6jKH21/NCH6qHqGCWFzpR9SVLlsiDecICAIRAwwIAhEDDAgCEQMMCAIRA6ALTBaGLKik1dKFq3r5Xap+lIntsqfWq5oWo1FQLFbro7e01Ne9nUgEJVVP7dqWUv/dUkf2sVGjCC8Koz1+FY7z1qge1tbURugAAxEXDAgCEQMMCAIRAwwIAhEDDAgCEQEoQ0wUpwSopNSWoeL+XVCJOJdpU8s07Vp2zyF5wR44cMTX1c6rjUkppeHjY1EZGRkzN+5nUGCr1M9XU1Mj1uef0Un7qWqX2FVKCAIDQaFgAgBBoWACAEGhYAIAQ9JtFAKgA9TI+d1yTp0hAQAUX1D0VCW2okUtFgggq4FFXV5d1Tu+e1M9f5DNVvPWTGdzjCQsAEAINCwAQAg0LABACDQsAEAKhCwBVNZkv7VVAQtW8gIHaJ0uFHlQQxNtjS03AUKELbz8tdS1lMidVVApPWACAEGhYAIAQaFgAgBBoWACAEGhYAIAQSAkCCCM3vVYkUacSgaOjo3J97j5XKpGn0oQp6TFQ6j6L7BFWJOU3VROBCk9YAIAQaFgAgBBoWACAEGhYAIAQZkV64QYAmLl4wgIAhEDDAgCEQMMCAIRAwwIAhEDDAgCEQMMCAIRAwwIAhEDDAgCEQMMCAIRAwwIAhEDDAgCEQMMCAIRAwwIAhEDDAgCEQMMCAIRAwwIAhEDDAgCEQMMCAIRAwwIAhEDDAgCEQMMCAIRAwwIAhEDDAgCEQMMCAIRAwwIAhEDDAgCE8D++QTuct0tFywAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 576x432 with 4 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_reconstructed_digits(X, outputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "图15-6：原始数字（左）和它们的重建（右）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "看起来十分相似。所以自动编码器已经学会了重现其输入，但是它是否学会了有用的特征？让我们来看看。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 特征可视化"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "一旦你的自动编码器学会了某些特征，你可能想看看它们。有各种技术可以满足你的愿望。其中，最简单的技术是考虑隐藏层的每个神经元，然后找到激活它最多的训练实例。该技术对顶层的隐藏层尤其有用，因为它们经常捕获相对大的特征，可以容易地从一组包含它们的训练实例中发现。例如，如果一个神经元在看到图片中有猫时被强烈激活，那么很明显激活它的照片大多数都包含猫。然而，对于比较低的隐藏层，该技术就不是很奏效，因为特征相对比较小而且更抽象，所以通常很难准确地了解神经元是如何被激活的。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们来看看另一种技术。对于第一个隐藏层的每个神经元，可以创建一个图像，其中每一个像素的强度代表了连接到给定神经元的权重。例如，如下代码展示了第一个隐藏层的5个神经元学到的特征："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./my_model_one_at_a_time.ckpt\n",
      "Saving figure extracted_features_plot\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 5 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    saver.restore(sess, \"./my_model_one_at_a_time.ckpt\") # not shown in the book\n",
    "    weights1_val = weights1.eval()\n",
    "\n",
    "for i in range(5):\n",
    "    plt.subplot(1, 5, i + 1)\n",
    "    plot_image(weights1_val.T[i])\n",
    "\n",
    "save_fig(\"extracted_features_plot\") # not shown\n",
    "plt.show()                          # not shown"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "图15-7：第一个隐藏层的神经元学习到的特征"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "前四个特征对应于小块特征，第五个特征在寻找垂直笔画（注意，这些特征来自于后面将要讨论的堆叠去噪自动编码器）。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "另外一种技术是向自动编码器馈送随机输入图像，测量你感兴趣的神经元的激活，然后执行反向传播，以使得神经元激活更多的方式调整输入图像。如果迭代多次（性能逐渐上升），图像逐渐变为令神经元兴奋的图像。这是一种非常有用的技术来可视化神经元寻找的输入类型。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "最终，如果使用自动编码器来执行无监督的预训练，例如分类器。一个简单验证自动编码器学习到的特征是否有用的方法是测量分类器的性能。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 使用堆叠的自动编码器进行无监控的预训练"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "正如我们在第11章所讨论的，如果正在执行一个复杂的监督任务，但是没有足够多的已标记训练数据，解决方案之一是找到一个执行类似任务的神经网络，然后重用它的底层。这样就可以使用较少的训练数据来训练高性能模型，因为你的神经网络不用学习所有的低层特征；它将重用现有网络的特征探测器。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "类似地，如果你有一个大数据集，但是大部分数据都没有被标记，可以首先使用所有数据训练一个堆叠的自动编码器，然后重用较低层来为你的实际任务创建一个神经网络，并使用已标记的数据来训练它。例如，图15-8显示了如何使用一个堆叠的自动编码器对分类神经网络进行无监督的预训练。如前所述，堆叠的自动编码器自身通常一次训练一个自动编码器。当训练分类器时，如果你实在没有太多的已标记训练数据，则可能需要冻结预训练层（至少是较低层）。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*这种情况很普遍，因为构建一个大的未标记的数据集通常比较容易（例如，一个简单的脚本可以从网络上下载数百万张图片），但是可靠的标记它们只能由人类完成（例如，将图片分为可爱和不可爱）。标记实例耗时而且昂贵，因此只有数千个已标记实例的情况非常普遍。*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='images/autoencoders/Unsupervised pretraining using autoencoders.png' >"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "图15-8：使用自动编码器进行无监督的预训练"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "正如我们之前讨论的，2006年Geoffrey Hinton等人的发现：深度神经网络可以被无监督的方式预处理，是触发目前深度学习海啸的因素之一。他们使用了限制性的Boltzmann机器来研究它（见附录E），但是在2007年，Yoshua Bengio等人的研究显示自动编码器也可以工作得一样好。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TensorFlow的实现相当简单：只需要用所有的训练数据训练一个自动编码器，然后复用它的编码层创建一个新的神经网络（更多关于如何复用预训练层的细节，请参见第11章，或者查看Jupyter笔记本中的代码示例）。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "至此，为了强制自动编码器学习有用的特征，我们限制了编码层的大小，使其不够完整。实际上我们可以使用一些其他类型的限制来得到一个完整的编码器，包括允许编码层和输入的大小一致，或者更大。我们下面来看一看这其中的一些方法。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's create a small neural network for MNIST classification:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "reset_graph()\n",
    "\n",
    "n_inputs = 28 * 28\n",
    "n_hidden1 = 300\n",
    "n_hidden2 = 150\n",
    "n_outputs = 10\n",
    "\n",
    "learning_rate = 0.01\n",
    "l2_reg = 0.0005\n",
    "\n",
    "activation = tf.nn.elu\n",
    "regularizer = tf.contrib.layers.l2_regularizer(l2_reg)\n",
    "initializer = tf.contrib.layers.variance_scaling_initializer()\n",
    "\n",
    "X = tf.placeholder(tf.float32, shape=[None, n_inputs])\n",
    "y = tf.placeholder(tf.int32, shape=[None])\n",
    "\n",
    "weights1_init = initializer([n_inputs, n_hidden1])\n",
    "weights2_init = initializer([n_hidden1, n_hidden2])\n",
    "weights3_init = initializer([n_hidden2, n_outputs])\n",
    "\n",
    "weights1 = tf.Variable(weights1_init, dtype=tf.float32, name=\"weights1\")\n",
    "weights2 = tf.Variable(weights2_init, dtype=tf.float32, name=\"weights2\")\n",
    "weights3 = tf.Variable(weights3_init, dtype=tf.float32, name=\"weights3\")\n",
    "\n",
    "biases1 = tf.Variable(tf.zeros(n_hidden1), name=\"biases1\")\n",
    "biases2 = tf.Variable(tf.zeros(n_hidden2), name=\"biases2\")\n",
    "biases3 = tf.Variable(tf.zeros(n_outputs), name=\"biases3\")\n",
    "\n",
    "hidden1 = activation(tf.matmul(X, weights1) + biases1)\n",
    "hidden2 = activation(tf.matmul(hidden1, weights2) + biases2)\n",
    "logits = tf.matmul(hidden2, weights3) + biases3\n",
    "\n",
    "cross_entropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y, logits=logits)\n",
    "reg_loss = regularizer(weights1) + regularizer(weights2) + regularizer(weights3)\n",
    "loss = cross_entropy + reg_loss\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate)\n",
    "training_op = optimizer.minimize(loss)\n",
    "\n",
    "correct = tf.nn.in_top_k(logits, y, 1)\n",
    "accuracy = tf.reduce_mean(tf.cast(correct, tf.float32))\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "pretrain_saver = tf.train.Saver([weights1, weights2, biases1, biases2])\n",
    "saver = tf.train.Saver()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Regular training (without pretraining):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 Train accuracy: 0.94666666 Test accuracy: 0.9265\n",
      "1 Train accuracy: 0.96666664 Test accuracy: 0.946\n",
      "2 Train accuracy: 0.97333336 Test accuracy: 0.9442\n",
      "3 Train accuracy: 0.97333336 Test accuracy: 0.9397\n"
     ]
    }
   ],
   "source": [
    "n_epochs = 4\n",
    "batch_size = 150\n",
    "n_labeled_instances = 20000\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    init.run()\n",
    "    for epoch in range(n_epochs):\n",
    "        n_batches = n_labeled_instances // batch_size\n",
    "        for iteration in range(n_batches):\n",
    "            print(\"\\r{}%\".format(100 * iteration // n_batches), end=\"\")\n",
    "            sys.stdout.flush()\n",
    "            indices = rnd.permutation(n_labeled_instances)[:batch_size]\n",
    "            X_batch, y_batch = mnist.train.images[indices], mnist.train.labels[indices]\n",
    "            sess.run(training_op, feed_dict={X: X_batch, y: y_batch})\n",
    "        accuracy_val = accuracy.eval(feed_dict={X: X_batch, y: y_batch})\n",
    "        print(\"\\r{}\".format(epoch), \"Train accuracy:\", accuracy_val, end=\" \")\n",
    "        saver.save(sess, \"./my_model_supervised.ckpt\")\n",
    "        accuracy_val = accuracy.eval(feed_dict={X: mnist.test.images, y: mnist.test.labels})\n",
    "        print(\"Test accuracy:\", accuracy_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now reusing the first two layers of the autoencoder we pretrained:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./my_model_cache_frozen.ckpt\n",
      "0 Train accuracy: 0.92\tTest accuracy: 0.9264\n",
      "1 Train accuracy: 0.9533333\tTest accuracy: 0.9169\n",
      "2 Train accuracy: 0.9533333\tTest accuracy: 0.948\n",
      "3 Train accuracy: 0.99333334\tTest accuracy: 0.9489\n"
     ]
    }
   ],
   "source": [
    "n_epochs = 4\n",
    "batch_size = 150\n",
    "n_labeled_instances = 20000\n",
    "\n",
    "#training_op = optimizer.minimize(loss, var_list=[weights3, biases3])  # Freeze layers 1 and 2 (optional)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    init.run()\n",
    "    pretrain_saver.restore(sess, \"./my_model_cache_frozen.ckpt\")\n",
    "    for epoch in range(n_epochs):\n",
    "        n_batches = n_labeled_instances // batch_size\n",
    "        for iteration in range(n_batches):\n",
    "            print(\"\\r{}%\".format(100 * iteration // n_batches), end=\"\")\n",
    "            sys.stdout.flush()\n",
    "            indices = rnd.permutation(n_labeled_instances)[:batch_size]\n",
    "            X_batch, y_batch = mnist.train.images[indices], mnist.train.labels[indices]\n",
    "            sess.run(training_op, feed_dict={X: X_batch, y: y_batch})\n",
    "        accuracy_val = accuracy.eval(feed_dict={X: X_batch, y: y_batch})\n",
    "        print(\"\\r{}\".format(epoch), \"Train accuracy:\", accuracy_val, end=\"\\t\")\n",
    "        saver.save(sess, \"./my_model_supervised_pretrained.ckpt\")\n",
    "        accuracy_val = accuracy.eval(feed_dict={X: mnist.test.images, y: mnist.test.labels})\n",
    "        print(\"Test accuracy:\", accuracy_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 去噪自动编码器 Stacked denoising Autoencoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "另一种强制自动编码器学习有用特征的方法是在输入中增加噪音，训练它以恢复原始的无噪音输入。这种方法阻止了自动编码器简单地复制其输入到输出，最终必须找到数据中的模式。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "自20世纪80年代以来，这种使用自动编码器去除噪音的方式就一直存在（例如，Yann LeCun硕士1987年发表的论文就已提到该自动编码器）。2008年，Pascal Vincent等人发表的论文表明，自动编码器也可以用于特征提取。2010年，Vincent等人的论文介绍了堆叠的去噪自动编码器。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "噪音可以是添加到输入中的纯高斯噪音，或者是随机打断输入的噪音，如dropout（第11章介绍的）。图15-9显示了上面的两种方法。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='images/autoencoders/Denoising autoencoders, with Gaussian noise (left) or dropout (right).png' >"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "图15-9：去噪自动编码器，高斯噪音（左）或dropout（右）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: the book uses `tf.contrib.layers.dropout()` rather than `tf.layers.dropout()` (which did not exist when this chapter was written). It is now preferable to use `tf.layers.dropout()`, because anything in the contrib module may change or be deleted without notice. The `tf.layers.dropout()` function is almost identical to the `tf.contrib.layers.dropout()` function, except for a few minor differences. Most importantly:\n",
    "* you must specify the dropout rate (`rate`) rather than the keep probability (`keep_prob`), where `rate` is simply equal to `1 - keep_prob`,\n",
    "* the `is_training` parameter is renamed to `training`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在TensorFlow实现去噪自动编码器不是很难。让我们从高斯噪音开始。除了为输入增加噪音和根据原始输入计算重建损坏之外，它和训练常规自动编码器很相似："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using Gaussian noise:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "reset_graph()\n",
    "\n",
    "n_inputs = 28 * 28\n",
    "n_hidden1 = 300\n",
    "n_hidden2 = 150  # codings\n",
    "n_hidden3 = n_hidden1\n",
    "n_outputs = n_inputs\n",
    "\n",
    "learning_rate = 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "noise_level = 1.0\n",
    "\n",
    "X = tf.placeholder(tf.float32, shape=[None, n_inputs])\n",
    "X_noisy = X + noise_level * tf.random_normal(tf.shape(X))\n",
    "\n",
    "hidden1 = tf.layers.dense(X_noisy, n_hidden1, activation=tf.nn.relu,\n",
    "                          name=\"hidden1\")\n",
    "hidden2 = tf.layers.dense(hidden1, n_hidden2, activation=tf.nn.relu, # not shown in the book\n",
    "                          name=\"hidden2\")                            # not shown\n",
    "hidden3 = tf.layers.dense(hidden2, n_hidden3, activation=tf.nn.relu, # not shown\n",
    "                          name=\"hidden3\")                            # not shown\n",
    "outputs = tf.layers.dense(hidden3, n_outputs, name=\"outputs\")        # not shown\n",
    "\n",
    "reconstruction_loss = tf.reduce_mean(tf.square(outputs - X)) # MSE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**因为X向量只在构建阶段定义，所以不能预知添加到X向量的噪音的向量**。我们不能调用X.get_shape（），因为这将只返回部分定义的X（[None，n_inputs]）向量，而random_normal（）返回一个完整定义的向量，所以它将引发异常。相反，我们调用tf.shape（X），它创建了一个在运行时返回该点完全定义的X向量的操作。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.train.AdamOptimizer(learning_rate)\n",
    "training_op = optimizer.minimize(reconstruction_loss)\n",
    "    \n",
    "init = tf.global_variables_initializer()\n",
    "saver = tf.train.Saver()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 Train MSE: 0.044186756\n",
      "1 Train MSE: 0.043341257\n",
      "2 %Train MSE: 0.042749457\n",
      "3 Train MSE: 0.04138295\n",
      "4 Train MSE: 0.041432336\n",
      "59% Train MSE: 0.041583534\n",
      "6 Train MSE: 0.040199198\n",
      "7 Train MSE: 0.040725287\n",
      "8 Train MSE: 0.040795565\n",
      "9 Train MSE: 0.038627468\n"
     ]
    }
   ],
   "source": [
    "n_epochs = 10\n",
    "batch_size = 150\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    init.run()\n",
    "    for epoch in range(n_epochs):\n",
    "        n_batches = mnist.train.num_examples // batch_size\n",
    "        for iteration in range(n_batches):\n",
    "            print(\"\\r{}%\".format(100 * iteration // n_batches), end=\"\")\n",
    "            sys.stdout.flush()\n",
    "            X_batch, y_batch = mnist.train.next_batch(batch_size)\n",
    "            sess.run(training_op, feed_dict={X: X_batch})\n",
    "        loss_train = reconstruction_loss.eval(feed_dict={X: X_batch})\n",
    "        print(\"\\r{}\".format(epoch), \"Train MSE:\", loss_train)\n",
    "        saver.save(sess, \"./my_model_stacked_denoising_gaussian.ckpt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using dropout:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "reset_graph()\n",
    "\n",
    "n_inputs = 28 * 28\n",
    "n_hidden1 = 300\n",
    "n_hidden2 = 150  # codings\n",
    "n_hidden3 = n_hidden1\n",
    "n_outputs = n_inputs\n",
    "\n",
    "learning_rate = 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "dropout_rate = 0.3\n",
    "\n",
    "training = tf.placeholder_with_default(False, shape=(), name='training')\n",
    "\n",
    "X = tf.placeholder(tf.float32, shape=[None, n_inputs])\n",
    "X_drop = tf.layers.dropout(X, dropout_rate, training=training)\n",
    "\n",
    "hidden1 = tf.layers.dense(X_drop, n_hidden1, activation=tf.nn.relu,\n",
    "                          name=\"hidden1\")\n",
    "hidden2 = tf.layers.dense(hidden1, n_hidden2, activation=tf.nn.relu, # not shown in the book\n",
    "                          name=\"hidden2\")                            # not shown\n",
    "hidden3 = tf.layers.dense(hidden2, n_hidden3, activation=tf.nn.relu, # not shown\n",
    "                          name=\"hidden3\")                            # not shown\n",
    "outputs = tf.layers.dense(hidden3, n_outputs, name=\"outputs\")        # not shown\n",
    "\n",
    "reconstruction_loss = tf.reduce_mean(tf.square(outputs - X)) # MSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.train.AdamOptimizer(learning_rate)\n",
    "training_op = optimizer.minimize(reconstruction_loss)\n",
    "    \n",
    "init = tf.global_variables_initializer()\n",
    "saver = tf.train.Saver()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在训练期间，必须使用feed_dict设置is_training的值为True（如第11章所解释的）："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 Train MSE: 0.027326608\n",
      "1 Train MSE: 0.025020054\n",
      "29% Train MSE: 0.022891767\n",
      "3 Train MSE: 0.022857895\n",
      "4 Train MSE: 0.02462086\n",
      "5 Train MSE: 0.023922253\n",
      "6 Train MSE: 0.02453716\n",
      "7 Train MSE: 0.022234337\n",
      "8 Train MSE: 0.023171421\n",
      "9 %Train MSE: 0.02333467\n"
     ]
    }
   ],
   "source": [
    "n_epochs = 10\n",
    "batch_size = 150\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    init.run()\n",
    "    for epoch in range(n_epochs):\n",
    "        n_batches = mnist.train.num_examples // batch_size\n",
    "        for iteration in range(n_batches):\n",
    "            print(\"\\r{}%\".format(100 * iteration // n_batches), end=\"\")\n",
    "            sys.stdout.flush()\n",
    "            X_batch, y_batch = mnist.train.next_batch(batch_size)\n",
    "            sess.run(training_op, feed_dict={X: X_batch, training: True})\n",
    "        loss_train = reconstruction_loss.eval(feed_dict={X: X_batch})\n",
    "        print(\"\\r{}\".format(epoch), \"Train MSE:\", loss_train)\n",
    "        saver.save(sess, \"./my_model_stacked_denoising_dropout.ckpt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在测试期间没有必要设置is_training为False，因为调用placeholder_with_default（）函数时我们设置其为默认值。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 稀疏自动编码器 Sparse Autoencoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "另一种导致良好特征提取的约束是稀疏性：通过在成本函数中增加适当的条件，推动自动编码器减小编码层中活动神经元的数量。例如，可能使得编码层只有平均5%的显著激活神经元。这迫使自动编码器使用少量激活神经元的组合来表示输入。结果，编码层的每个神经元都最终代表一个有用特征（如果你每个月只能说几个单词，那么你可能会尝试让它们变得有意义）。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "为了有利于稀疏性模型，必须首先测量每个训练迭代编码层的实际稀疏度。我们通过计算整个训练批次中编码层每个神经元的平均激活程度来实现。批次的尺寸不能太小，否则不能准确计算平均值。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "一旦得到了每个神经元的平均激活度，我们希望通过给成本函数添加稀疏性损失以削弱过度激活的神经元。例如，如果测量得到一个神经元的平均激活度是0.3，但是目标稀疏度是0.1，则必须削弱其激活度。一种简单的方法是通过给成本函数增加平方误差 $（0.3–0.1）^2$，但是实际上更好的方法是使用Kullback-Leibler散度（第4章简要讨论过），如图15-10所示，它比均方误差具有更强的梯度。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "给定两个离散概率分布P和Q，可以使用公式15-1计算该分布之间的KL散度，表示为DKL（P||Q）。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving figure sparsity_loss_plot\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "p = 0.1\n",
    "q = np.linspace(0.001, 0.999, 500)\n",
    "kl_div = p * np.log(p / q) + (1 - p) * np.log((1 - p) / (1 - q))\n",
    "mse = (p - q)**2\n",
    "plt.plot([p, p], [0, 0.3], \"k:\")\n",
    "plt.text(0.05, 0.32, \"Target\\nsparsity\", fontsize=14)\n",
    "plt.plot(q, kl_div, \"b-\", label=\"KL divergence\")\n",
    "plt.plot(q, mse, \"r--\", label=\"MSE\")\n",
    "plt.legend(loc=\"upper left\")\n",
    "plt.xlabel(\"Actual sparsity\")\n",
    "plt.ylabel(\"Cost\", rotation=0)\n",
    "plt.axis([0, 1, 0, 0.95])\n",
    "save_fig(\"sparsity_loss_plot\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "公式15-1：Kullback–Leibler散度"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='images/autoencoders/Kullback–Leibler divergence.png' >"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在本例中，我们想要计算神经元在编码层被激活的目标概率p，及实际概率q（即，训练批次的平均激活度）之间的差距。所以，KL散度可以简化为公式15-2。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "公式15-2：目标稀疏度p和实际稀疏度q之间的KL散度"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='images/autoencoders/KL divergence between the target sparsity p and the actual sparsity q.png' >"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "一旦计算了编码层每个神经元的稀疏度损失，我们只需要累加这些损失，然后将其结果加到成本函数中。为了控制稀疏度损伤和重建损失的相对重要性，我们可以通过一个稀疏度权重超参数来增加稀疏度损失。如果这个权重过高，模型将非常接近目标稀疏度，但是可能不能正确地重建输入，使得模型无用。相反，如果它的值过低，模型将忽略大多数稀疏性目标，并且学习不到什么有用特征。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "reset_graph()\n",
    "\n",
    "n_inputs = 28 * 28\n",
    "n_hidden1 = 1000  # sparse codings\n",
    "n_outputs = n_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def kl_divergence(p, q):\n",
    "    # Kullback Leibler divergence\n",
    "    return p * tf.log(p / q) + (1 - p) * tf.log((1 - p) / (1 - q))\n",
    "\n",
    "learning_rate = 0.01\n",
    "sparsity_target = 0.1\n",
    "sparsity_weight = 0.2\n",
    "\n",
    "X = tf.placeholder(tf.float32, shape=[None, n_inputs])            # not shown in the book\n",
    "\n",
    "hidden1 = tf.layers.dense(X, n_hidden1, activation=tf.nn.sigmoid) # not shown\n",
    "outputs = tf.layers.dense(hidden1, n_outputs)                     # not shown\n",
    "\n",
    "hidden1_mean = tf.reduce_mean(hidden1, axis=0) # batch mean\n",
    "sparsity_loss = tf.reduce_sum(kl_divergence(sparsity_target, hidden1_mean))\n",
    "reconstruction_loss = tf.reduce_mean(tf.square(outputs - X)) # MSE\n",
    "loss = reconstruction_loss + sparsity_weight * sparsity_loss\n",
    "\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate)\n",
    "training_op = optimizer.minimize(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "init = tf.global_variables_initializer()\n",
    "saver = tf.train.Saver()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 Train MSE: 0.13315204 \tSparsity loss: 0.15598294 \tTotal loss: 0.16434863\n",
      "1 Train MSE: 0.060101468 \tSparsity loss: 0.03877146 \tTotal loss: 0.06785576\n",
      "2 Train MSE: 0.0524878 \tSparsity loss: 0.06478515 \tTotal loss: 0.065444835\n",
      "3 Train MSE: 0.047601074 \tSparsity loss: 0.010397203 \tTotal loss: 0.049680516\n",
      "4 Train MSE: 0.042802855 \tSparsity loss: 0.3538871 \tTotal loss: 0.11358028\n",
      "5 Train MSE: 0.040932737 \tSparsity loss: 0.01388924 \tTotal loss: 0.043710586\n",
      "6 Train MSE: 0.039479308 \tSparsity loss: 0.04311932 \tTotal loss: 0.048103172\n",
      "7 Train MSE: 0.037478052 \tSparsity loss: 0.29010892 \tTotal loss: 0.095499836\n",
      "8 Train MSE: 0.033432372 \tSparsity loss: 0.272291 \tTotal loss: 0.08789057\n",
      "9 Train MSE: 0.030924855 \tSparsity loss: 0.020251643 \tTotal loss: 0.034975182\n",
      "10 Train MSE: 0.028239414 \tSparsity loss: 0.05163889 \tTotal loss: 0.038567193\n",
      "11 Train MSE: 0.025655719 \tSparsity loss: 0.32736695 \tTotal loss: 0.09112911\n",
      "12 Train MSE: 0.024245808 \tSparsity loss: 0.03820443 \tTotal loss: 0.031886693\n",
      "13 Train MSE: 0.022193884 \tSparsity loss: 0.097674325 \tTotal loss: 0.04172875\n",
      "14 Train MSE: 0.02180774 \tSparsity loss: 0.06191695 \tTotal loss: 0.03419113\n",
      "15 Train MSE: 0.020155331 \tSparsity loss: 0.021000804 \tTotal loss: 0.024355492\n",
      "16 Train MSE: 0.01949621 \tSparsity loss: 0.089586616 \tTotal loss: 0.037413534\n",
      "17 Train MSE: 0.018494673 \tSparsity loss: 0.05963824 \tTotal loss: 0.030422322\n",
      "18 Train MSE: 0.017803738 \tSparsity loss: 0.037217528 \tTotal loss: 0.025247244\n",
      "19 Train MSE: 0.016793605 \tSparsity loss: 0.10370506 \tTotal loss: 0.037534617\n",
      "20 Train MSE: 0.016702581 \tSparsity loss: 0.30384254 \tTotal loss: 0.07747109\n",
      "21 Train MSE: 0.016601417 \tSparsity loss: 0.19870096 \tTotal loss: 0.05634161\n",
      "22 Train MSE: 0.016437799 \tSparsity loss: 0.022708911 \tTotal loss: 0.020979581\n",
      "23 Train MSE: 0.016890189 \tSparsity loss: 0.094476126 \tTotal loss: 0.035785414\n",
      "24 Train MSE: 0.015758464 \tSparsity loss: 0.055750437 \tTotal loss: 0.02690855\n",
      "25 Train MSE: 0.015040976 \tSparsity loss: 0.03893327 \tTotal loss: 0.022827629\n",
      "26 Train MSE: 0.014288768 \tSparsity loss: 0.037836876 \tTotal loss: 0.021856144\n",
      "27 Train MSE: 0.01599095 \tSparsity loss: 0.18066159 \tTotal loss: 0.052123267\n",
      "28 Train MSE: 0.0144590195 \tSparsity loss: 0.054316726 \tTotal loss: 0.025322365\n",
      "29 Train MSE: 0.013808356 \tSparsity loss: 0.04669548 \tTotal loss: 0.023147453\n",
      "30 Train MSE: 0.0140096685 \tSparsity loss: 0.06442584 \tTotal loss: 0.026894838\n",
      "31 Train MSE: 0.013033918 \tSparsity loss: 0.11439811 \tTotal loss: 0.03591354\n",
      "32 Train MSE: 0.013262496 \tSparsity loss: 0.079256475 \tTotal loss: 0.029113792\n",
      "33 Train MSE: 0.012965736 \tSparsity loss: 0.07439161 \tTotal loss: 0.027844058\n",
      "34 Train MSE: 0.01366256 \tSparsity loss: 0.08912538 \tTotal loss: 0.031487636\n",
      "35 Train MSE: 0.013719946 \tSparsity loss: 0.120539464 \tTotal loss: 0.037827842\n",
      "36 Train MSE: 0.01341205 \tSparsity loss: 0.056773014 \tTotal loss: 0.024766654\n",
      "37 Train MSE: 0.012816419 \tSparsity loss: 0.03965255 \tTotal loss: 0.020746928\n",
      "38 Train MSE: 0.014445931 \tSparsity loss: 0.11553084 \tTotal loss: 0.0375521\n",
      "39 Train MSE: 0.012532197 \tSparsity loss: 0.089181185 \tTotal loss: 0.030368434\n",
      "40 Train MSE: 0.012928332 \tSparsity loss: 0.053560212 \tTotal loss: 0.023640376\n",
      "41 Train MSE: 0.013434295 \tSparsity loss: 0.11425727 \tTotal loss: 0.03628575\n",
      "42 Train MSE: 0.0126154795 \tSparsity loss: 0.37438148 \tTotal loss: 0.08749178\n",
      "43 Train MSE: 0.012749081 \tSparsity loss: 0.0787471 \tTotal loss: 0.0284985\n",
      "44 Train MSE: 0.013693485 \tSparsity loss: 0.19888368 \tTotal loss: 0.053470224\n",
      "45 Train MSE: 0.012656051 \tSparsity loss: 0.05376394 \tTotal loss: 0.02340884\n",
      "46 Train MSE: 0.0128339 \tSparsity loss: 0.07454881 \tTotal loss: 0.027743662\n",
      "47 Train MSE: 0.011900017 \tSparsity loss: 0.0576417 \tTotal loss: 0.023428358\n",
      "48 Train MSE: 0.0121138645 \tSparsity loss: 0.09634711 \tTotal loss: 0.031383287\n",
      "49 Train MSE: 0.012640937 \tSparsity loss: 0.06588787 \tTotal loss: 0.025818512\n",
      "50 Train MSE: 0.011677614 \tSparsity loss: 0.07750493 \tTotal loss: 0.0271786\n",
      "51 Train MSE: 0.011538501 \tSparsity loss: 0.08859773 \tTotal loss: 0.029258046\n",
      "52 Train MSE: 0.011719111 \tSparsity loss: 0.10398395 \tTotal loss: 0.032515902\n",
      "53 Train MSE: 0.011549973 \tSparsity loss: 0.08787192 \tTotal loss: 0.029124357\n",
      "54 Train MSE: 0.016429724 \tSparsity loss: 0.71485585 \tTotal loss: 0.1594009\n",
      "55 Train MSE: 0.011618633 \tSparsity loss: 0.07256577 \tTotal loss: 0.026131786\n",
      "36%"
     ]
    }
   ],
   "source": [
    "n_epochs = 100\n",
    "batch_size = 1000\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    init.run()\n",
    "    for epoch in range(n_epochs):\n",
    "        n_batches = mnist.train.num_examples // batch_size\n",
    "        for iteration in range(n_batches):\n",
    "            print(\"\\r{}%\".format(100 * iteration // n_batches), end=\"\")\n",
    "            sys.stdout.flush()\n",
    "            X_batch, y_batch = mnist.train.next_batch(batch_size)\n",
    "            sess.run(training_op, feed_dict={X: X_batch})\n",
    "        reconstruction_loss_val, sparsity_loss_val, loss_val = sess.run([reconstruction_loss, sparsity_loss, loss], feed_dict={X: X_batch})\n",
    "        print(\"\\r{}\".format(epoch), \"Train MSE:\", reconstruction_loss_val, \"\\tSparsity loss:\", sparsity_loss_val, \"\\tTotal loss:\", loss_val)\n",
    "        saver.save(sess, \"./my_model_sparse.ckpt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
